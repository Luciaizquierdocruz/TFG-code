---
title: "GRU Tesla"
author: "Lucía Izquierdo"
date: "2025-06-26"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Entorno de Python

Se establece el entorno de python descragado.

```{r}
library(reticulate)

# Establece manualmente la ruta al ejecutable de Python 3.10
use_python("C:/Program Files/Python310/python.exe", required = TRUE)

# Comprueba que está usándose correctamente
py_config()
```

# Carga de Librerias Python

Previamente descargadas con el cdm del propio ordenador. Por ejemplo: pip install pandas

```{python}
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import TensorDataset, DataLoader
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error


device = torch.device("cuda" if torch.cuda.is_available() else "cpu") 
#Entrenar modelos con redes neuronale es mucho más rápido en una GPU que en CPU. Por eso PyTorch permite mover los datos y modelos a ese dispositivo.
#Si tienes una GPU compatible las usará,si no, usará cpu, es decir, el procesador normal.
```

# Carga de Librerias de R

```{r}
library(quantmod)
library(ggplot2)
```


# Carga de datos 

```{r}
getSymbols("TSLA", from = "2011-01-01", to = "2024-12-31")
data <- TSLA[, "TSLA.Close"]
colnames(data) <- "Close"


write.zoo(data, file = "data_close.csv", sep = ",", col.names = TRUE, index.name = "Date")
#Crea un csv en el directorio donde esta el archivo, para poder leerlo luego desde python
```

```{python}
dataset = pd.read_csv("data_close.csv",index_col='Date', parse_dates=['Date'])
```

# Preprocesamiento de los datos 

**Min-Max Normalization**

```{python}
dataset_norm = dataset.copy()
scaler = MinMaxScaler()
dataset_norm['Close'] = scaler.fit_transform(dataset[['Close']])
```

**particion en test y train**

```{python}
dataset.index = pd.to_datetime(dataset.index)

train = dataset_norm[dataset_norm.index < '2024-01-01']
test = dataset_norm[dataset_norm.index >= '2024-01-01']
```

# Sliding Windows 

```{python}
# Valor inicial del lag
lag = 2

# Función para crear ventanas deslizantes
def create_sliding_windows(data, len_data, lag):
    x = []
    y = []
    for i in range(lag, len_data):
        x.append(data[i-lag:i, 0])
        y.append(data[i, 0])
    return np.array(x), np.array(y)

# Convertir los conjuntos de entrenamiento y prueba a arrays numpy
array_training_set = np.array(train)
array_test_set = np.array(test)

# Crear ventanas deslizantes para los datos de entrenamiento
x_train, y_train = create_sliding_windows(array_training_set, len(array_training_set), lag)
x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))

# Crear ventanas deslizantes para los datos de prueba
x_test, y_test = create_sliding_windows(array_test_set, len(array_test_set), lag)
x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1)) #(n_muestras, pasos_temporales, n_variables)
```

# Modelo GRU

```{python}
# Convertir datos a tensores PyTorch
x_train_tensor = torch.tensor(x_train, dtype=torch.float32).to(device)
y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)
x_test_tensor = torch.tensor(x_test, dtype=torch.float32).to(device)
y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)

# Dataset y DataLoader
train_dataset = TensorDataset(x_train_tensor, y_train_tensor)
train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)

# Modelo GRU en PyTorch
class GRUModel(nn.Module):
    def __init__(self, input_size=1, hidden_size=64, num_layers=3, dropout=0.2):
        super(GRUModel, self).__init__()
        self.gru = nn.GRU(input_size, hidden_size, num_layers, 
                          batch_first=True, dropout=dropout)
        self.fc = nn.Linear(hidden_size, 1)

    def forward(self, x):
        output, _ = self.gru(x)
        output = output[:, -1, :]  # solo el último paso temporal
        out = self.fc(output)
        return out

model = GRUModel().to(device)
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)
```

# Entrenamiento

```{python}
n_epochs = 100
model.train()
for epoch in range(n_epochs):
    epoch_loss = 0
    for xb, yb in train_loader:
        optimizer.zero_grad()
        output = model(xb)
        loss = criterion(output.squeeze(), yb)
        loss.backward()
        optimizer.step()
        epoch_loss += loss.item()
    if epoch % 10 == 0 or epoch == n_epochs - 1:
        print(f"Epoch {epoch+1}/{n_epochs}, Loss: {epoch_loss:.6f}")
```
# Predicción 

```{python}
# Predicción sobre test
model.eval()
with torch.no_grad():
    y_pred_test = model(x_test_tensor).cpu().numpy()

# Desnormalizar
y_pred_invert_norm = scaler.inverse_transform(y_pred_test)

# Desnormalizar valores reales (y_test)
y_test_reshaped = y_test.reshape(-1, 1)
y_test_invert_norm = scaler.inverse_transform(y_test_reshaped)
```

# Comparación prediccicón con el data set 

```{python}
datatest = y_test_invert_norm
datapred = y_pred_invert_norm
```

**Metricas**

```{python}
# Función para calcular RMSE
def rmse(y_true, y_pred):
    return np.round(np.sqrt(np.mean((y_pred - y_true) ** 2)), 4)

def mape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    mask = y_true != 0
    return np.round(np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100, 4)

mae_value = np.round(mean_absolute_error(datatest, datapred), 4)
mse_value = np.round(mean_squared_error(datatest, datapred), 4)
rmse_value = rmse(datatest, datapred)
mape_value = mape(datatest, datapred)

# Línea resumen con todos los resultados
print(f"\nResumen de métricas -> MAE: {mae_value} | MSE: {mse_value} | RMSE: {rmse_value} | MAPE: {mape_value}%")

```

**Grafico preds vs obs**

```{python}
datacompare = pd.DataFrame()
datacompare['Data Test'] = datatest.flatten()
datacompare['Prediction Results'] = datapred.flatten()

# Fechas correspondientes
totaldatatrain = len(train)
fechas_pred = dataset.index[totaldatatrain + lag : totaldatatrain + lag + len(y_test)]
datacompare['Date'] = fechas_pred
```

```{r}
datacompare <- py$datacompare
# Crear gráfico de comparación: Datos reales vs predicciones
ggplot(datacompare, aes(x = Date)) +
  geom_line(aes(y = `Data Test`, color = "Datos Reales"), linewidth = 1) +
  geom_line(aes(y = `Prediction Results`, color = "Predicciones"), linewidth = 1) +
  labs(title = "Precio de cierre de Tesla: predicción vs real",
       x = "Fecha",
       y = "Precio",
       color = "") +
  theme_minimal() +
  theme(legend.position = "bottom")

```